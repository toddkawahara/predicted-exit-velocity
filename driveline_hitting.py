# -*- coding: utf-8 -*-
"""Driveline Hitting.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YFewbE7M4gKUNxJ3MArdWnY9UtTZpKe0
"""

# Importing packages
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Changing display
pd.set_option('display.width', 1000)
pd.set_option('display.max_columns', 127)
pd.set_option('display.max_rows', 100)

"""#Loading and merging data"""

# Loading in data
poi = pd.read_csv('/content/drive/MyDrive/Datasets/hitting biomechanics/poi_metrics.csv')
hittrax = pd.read_csv('/content/drive/MyDrive/Datasets/hitting biomechanics/hittrax.csv')

# Selecting relevant data from hittrax
hittrax_data = hittrax[['session_swing', 'pitch_angle', 'la']]

# Merging data
data = poi.merge(hittrax_data, on='session_swing')

# Checking how many null entries
data.isnull().sum().sum()

# Dropping null entries
data.dropna(inplace=True)

data.shape

data.head()

data.info(verbose=True)

"""#Making angle difference variable"""

# Visualizing attack angle distribution
fig, (ax1, ax2) = plt.subplots(1,2, figsize=(15,4))
sns.histplot(data['attack_angle_contact_x'], ax=ax1, kde=True, legend=False)
data.boxplot(column=['attack_angle_contact_x'], ax=ax2)
plt.show()

# Creating inverse of attack angle
data['attack_angle_inverse'] = data['attack_angle_contact_x'] * -1

# Creating angle difference variable by measuring difference between the attack and pitch angle
data['angle_difference'] = data['attack_angle_inverse'] - data['pitch_angle']

# Visualizing angle difference distribution
sns.histplot(data['angle_difference'], kde=True, legend=False)
plt.show()

# Visualizing relationship between angle difference and exit velo
sns.regplot(data=data, x='angle_difference', y='exit_velo_mph_x')

# Visualizing relationship between angle difference and launch angle
sns.regplot(data=data, x='angle_difference', y='la')

# Examining features most correlated with launch angle, angle difference more correlated than any other!
la_corr = abs(data.corr()).sort_values(by='la', ascending=False)
la_corr.iloc[0].sort_values(ascending=False).head()

"""#Exit Velo

##EDA and Data Cleaning
"""

# Generating list of features in order of highest correlation with exit velocity
corr_ev = abs(data.corr()).sort_values(by='exit_velo_mph_x', ascending=False)

# Reducing feature set to 50 highest correlations with exit velo
ev_cols = corr_ev.iloc[0].sort_values(ascending=False).head(50).axes[0].to_list()

# New exit velo dataset with the reduced feature set
ev_data = data[ev_cols]

# Examining distributions for each feature
for x in ev_data.columns:
  fig, (ax1,ax2) = plt.subplots(1,2, figsize=(15,4))
  # Histogram
  sns.histplot(ev_data[x], ax=ax1, kde=True, legend=False)
  # Box plot
  ev_data.boxplot(column=[x], ax=ax2)
  plt.show()

## Removing outliers from the exit velo distribution

# Getting original amount of observations
original_rows = ev_data.shape[0]

# Getting inner quartile range
Q1 = ev_data['exit_velo_mph_x'].quantile(0.25)
Q3 = ev_data['exit_velo_mph_x'].quantile(0.75)
IQR = Q3 - Q1

# Removing outliers from the exit velo dataset
ev_data = ev_data[~((ev_data['exit_velo_mph_x'] < (Q1 - 1.5 * IQR)) |(ev_data['exit_velo_mph_x'] > (Q3 + 1.5 * IQR)))]

# Creating another dataset with only the outliers that were removed
outlier_data = data[((data['exit_velo_mph_x'] < (Q1 - 1.5 * IQR)) |(data['exit_velo_mph_x'] > (Q3 + 1.5 * IQR)))]

# Seeing how many outliers were removed
outliers_removed = original_rows - ev_data.shape[0]
print("Outliers removed: ", outliers_removed)

# Visualizing distribution of exit velos after removing outliers
fig, (ax1,ax2) = plt.subplots(1,2, figsize=(15,4))

# Histogram
sns.histplot(ev_data['exit_velo_mph_x'], ax=ax1, kde=True, legend=False)
# Box plot
ev_data.boxplot(column=['exit_velo_mph_x'], ax=ax2)

# Summary stats of dataset
ev_data.describe()

# Correlation heatmap of dataset
sns.heatmap(ev_data.corr())

# Examining shape
ev_data.shape

## Creating feature set that removes some redundant information and reduces features even further by removing features that have correlations greater than 0.5 with another, code obtained from
## https://stackoverflow.com/questions/29294983/how-to-calculate-correlation-between-all-columns-and-remove-highly-correlated-on

# Creating a copy of dataset
less_cols = ev_data.copy()

# Create correlation matrix
corr_matrix = less_cols.corr().abs()

# Select upper triangle of correlation matrix
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))

# Find features with correlation greater than 0.5
to_drop = [column for column in upper.columns if any(upper[column] > 0.5)]

# Drop features
less_cols.drop(to_drop, axis=1, inplace=True)

# Examining new shape
less_cols.shape

# Correlation heatmap of reduced features
sns.heatmap(less_cols.corr())

# Generating list of new features
less_cols_list = less_cols.columns.to_list()

# Removing target variable
less_cols_list.remove('exit_velo_mph_x')
less_cols_list

# Removing target variable from bigger feature set as well
ev_cols.remove('exit_velo_mph_x')

"""##Baseline Model"""

# Creating baseline dummy regressor model
from sklearn.dummy import DummyRegressor
dummy_regr = DummyRegressor(strategy="mean")

# Fitting and predicting
dummy_regr.fit(ev_data[ev_cols], ev_data['exit_velo_mph_x'])
dummy_preds = dummy_regr.predict(ev_data[ev_cols])

# Evaluating baseline model

from sklearn.metrics import mean_squared_error
rmse = mean_squared_error(y_true = ev_data['exit_velo_mph_x'], y_pred = dummy_preds)
print("Dummy RMSE:", rmse)

"""##Initial Models"""

# Importing packages for predictions
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor

from sklearn.model_selection import cross_val_score

# Evaluating Random Forest model
rf_ev_initial = RandomForestRegressor()

# Evaulating predictions from smaller feature set
print("Fewer cols:", cross_val_score(rf_ev_initial, ev_data[less_cols_list], ev_data['exit_velo_mph_x'], cv=5,
                                       scoring='neg_root_mean_squared_error').mean())
# Evaluating predictions from larger feature set
print("More cols:", cross_val_score(rf_ev_initial, ev_data[ev_cols], ev_data['exit_velo_mph_x'], cv=5,
                                       scoring='neg_root_mean_squared_error').mean())

# Evaluating XGBoost model
xgb_ev_initial = XGBRegressor()

# Evaluating predictions from smaller feature set
print("Fewer cols:", cross_val_score(xgb_ev_initial, ev_data[less_cols_list], ev_data['exit_velo_mph_x'], cv=5,
                                       scoring='neg_root_mean_squared_error').mean())
# Evaluating predictions from larger feature set
print("More cols:", cross_val_score(xgb_ev_initial, ev_data[ev_cols], ev_data['exit_velo_mph_x'], cv=5,
                                       scoring='neg_root_mean_squared_error').mean())

# Evaulating Linear Regression model
lr_ev_initial = LinearRegression()

# Evaluating predictions from smaler feature set (didn't use larger feature set because of multicollinearity)
print(cross_val_score(lr_ev_initial, ev_data[less_cols_list], ev_data['exit_velo_mph_x'], cv=5, scoring = 'neg_root_mean_squared_error').mean())

"""##Tuning RF"""

# Importing grid searches for tuning
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV

## Tuning Random Forest model

# Initializing model
rf = RandomForestRegressor()

# Creating parameter grid
param_grid = {
 'bootstrap': [True, False],
 'max_depth': [10, 20, 50, 100, 150, 200, None],
 'max_features': [2, 5, 10, 20, 'auto', 'sqrt'],
 'min_samples_leaf': [2, 5, 10, 20],
 'min_samples_split': [2, 5, 10, 20],
 'n_estimators': [50, 100, 200, 500, 1000, 1500, 2000]
}

# Creating randomized grid search
rnd_rf = RandomizedSearchCV(rf, param_distributions = param_grid, n_iter = 300, cv = 3, verbose = True, n_jobs = -1)
# Fitting grid search
rnd_rf.fit(ev_data[ev_cols], ev_data['exit_velo_mph_x'])

# Printing randomized grid search best parameters
print(rnd_rf.best_params_)

# Creating new parameter grid from randomized grid search results
param_grid = {
 'bootstrap': [True],
 'max_depth': [4, 8, 12],
 'max_features': [20, 30, 50],
 'min_samples_leaf': [1, 2, 3],
 'min_samples_split': [4, 5, 7],
 'n_estimators': [75, 100, 125]
}

# Creating grid search
grid_rf = GridSearchCV(rf, param_grid = param_grid, cv = 3, verbose = 2, n_jobs = -1)
# Fitting grid search
grid_rf.fit(ev_data[ev_cols], ev_data['exit_velo_mph_x'])

# Printing grid search best parameters
print(grid_rf.best_params_)

# Creating Random Forest model with tuned hyperparameters
rf = RandomForestRegressor(bootstrap=True, max_depth=12, max_features=20, min_samples_leaf=3, min_samples_split=5, n_estimators=75)

# Evaulating tuned Random Forest Model
print("RMSE:", cross_val_score(rf, ev_data[ev_cols], ev_data['exit_velo_mph_x'], cv=5,
                                       scoring='neg_root_mean_squared_error').mean())

"""##Generating Predictions"""

# Importing package for predicting
from sklearn.model_selection import cross_val_predict

# Generating predictions
preds = cross_val_predict(rf, ev_data[ev_cols], ev_data['exit_velo_mph_x'], cv=5)

# Concatenating predictions to dataset
preds = pd.Series(preds, index = ev_data.index)
ev_data['predicted_ev'] = preds

## Generating predictions for the outlier exit velocity sample

# Creating same feature set of exit velocity dataset for the outlier dataset
outlier_cols = ev_data.columns.to_list()
outlier_cols.remove('predicted_ev')

# Setting outlier dataset to only contain features
outlier_data = outlier_data[outlier_cols]

# Fitting Random Forest model to the non-outlier data
rf.fit(ev_data[ev_cols], ev_data['exit_velo_mph_x'])

# Predicting the outlier exit velocities
outlier_preds = rf.predict(outlier_data[ev_cols])

# Concatenating predictions to the outlier dataset
outlier_preds = pd.Series(outlier_preds, index=outlier_data.index)
outlier_data['predicted_ev'] = outlier_preds

# Concatenating the exit velocity and outlier datasets
ev_data = pd.concat([ev_data, outlier_data])

"""##Evaluating Model"""

## Plotting predictions against the actual exit velocities
f, ax = plt.subplots(figsize=(10, 5))

# Scatter plot
ax.scatter(ev_data['predicted_ev'], ev_data['exit_velo_mph_x'])

# Setting labels
plt.ylabel("Actual Exit Velo")
plt.xlabel("Predicted Exit Velo")

# Setting view
plt.ylim([45, 120])
plt.xlim([45, 120])

# Plotting line of identity
ax.plot([0, 1], [0, 1], transform=ax.transAxes)

# Plotting line of best fit
plt.plot(np.unique(ev_data['predicted_ev']), np.poly1d(np.polyfit(ev_data['predicted_ev'],
                                                                  ev_data['exit_velo_mph_x'], 1))(np.unique(ev_data['predicted_ev'])))

plt.show()

## Scatter plot of predictions against actual more focused on data the model was trained on

f, ax = plt.subplots(figsize=(10, 5))

# Scatter plot
ax.scatter(ev_data['predicted_ev'], ev_data['exit_velo_mph_x'])

# Setting labels
plt.ylabel("Actual Exit Velo")
plt.xlabel("Predicted Exit Velo")

# More focused on exit velocities model was trained on
plt.ylim([70, 110])
plt.xlim([70, 110])

# Plotting line of identity
ax.plot([0, 1], [0, 1], transform=ax.transAxes)

# Plotting line of best fit
plt.plot(np.unique(ev_data['predicted_ev']), np.poly1d(np.polyfit(ev_data['predicted_ev'],
                                                                  ev_data['exit_velo_mph_x'], 1))(np.unique(ev_data['predicted_ev'])))

plt.show()

# Summary statistics of actual exit velocities against predicted
ev_data[['exit_velo_mph_x', 'predicted_ev']].describe()

# Distributions of actual and predicted exit velocities
fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (13,4))

# Plotting histograms
bins = np.linspace(40, 110, 35)
sns.histplot(ev_data['exit_velo_mph_x'], ax=ax1, kde=True, legend=False, bins=bins)
sns.histplot(ev_data['predicted_ev'], ax=ax2, kde=True, legend=False, bins=bins)

# Changing view
ax1.set_xlim(left=45, right=110)
ax2.set_xlim(left=45, right=110)
ax1.set_ylim(bottom=0, top=120)
ax2.set_ylim(bottom=0, top=120)

# Setting labels
ax1.set_title('Acutal Exit Velo')
ax2.set_title('Predicted Exit Velo')
ax1.set_xlabel('Exit Velo')
ax2.set_xlabel('Exit Velo')

plt.show()

# Plotting the distributions overlayed
bins = np.linspace(40, 110, 35)

# Actual distribution
sns.histplot(ev_data['exit_velo_mph_x'], color='g', bins=bins, alpha=0.3, label="Actual Exit Velo")

# Predicted distribution
sns.histplot(ev_data['predicted_ev'], color = 'b', bins=bins, alpha=0.3, label='Predicted Exit Velo')

# Creating labels
plt.legend()
plt.xlabel('Exit Velo')

plt.show()

## Generating feature importances

# Getting the 9 most importance features
features = ev_cols[:9]
importances = rf.feature_importances_[:9]
# Sorting features by importance level
indices = np.argsort(importances)

# Plotting the feature importances
plt.barh(range(len(indices)), importances[indices], color='b', align='center')
plt.yticks(range(len(indices)), [features[i] for i in indices])

# Creating labels
plt.title('Feature Importances')
plt.xlabel('Relative Importance')

plt.show()